\chapter{Guaranteeing safety}

\input{60-tbl-safety-checks}


Copied from introduction
\begin{enumerate}
	\item write to memory outside the range assigned by the VM
	\item perform actions it does not have permission for
	\item retain control of the CPU indefinitely
\end{enumerate}


The complexity of reducing the necessary run-time checks noted by Harbor, is much lower when using a virtual machine. The JVM instruction set is relatively simple and restricted, compared to native CPU instruction sets, and is much easier to reason about. As we will see, many checks can be done at load time, reducing the need for runtime checks and the corresponding overhead.

We want to guarantee malicious code can't (i) write to memory not assigned to it by the VM, (ii) perform actions it's not allowed to, and (iii) retain the CPU indefinitely. Given the first two, the last guarantee is easy to implement: the VM can simply set a timer to trap back to the VM after a certain amount of time. As long as the second guarantee holds, no application will be able to disable or reset the timer without the VM's permission.

For an interpreter, the VM is always in control, so the other two guarantees are relatively easy to implement as well, but interpreters come at the cost of a one to two orders of magnitude performance penalty. When using AOT compilation, the application is executing natively most of the time, but we have the advantage that this code is generated by the VM, and the VM can do checks at translation time to ensure the code it generates is safe. Only in cases where this is impossible to guarantee do we need to insert an expensive run-time check.

To guarantee safety we will first make the two guarantees more concrete and specific to our VM:

\begin{itemize}
	\item \emph{control flow safety}: we are always executing
		\begin{itemize}
			\item a translated JVM instruction from the top (so code can't jump to half way a generated instruction with undefined results), or
			\item code in the VM itself, as a result of either a call to the VM from a translated JVM instruction, or returning from the main method
		\end{itemize}
	\item \emph{memory safety}: any memory access done by the application is to a legal location:
		\begin{itemize}
			\item memory reserved for the operand stack, or
			\item a valid local or static variable, or
			\item the area of the heap assigned to the application
		\end{itemize}
\end{itemize}

Both depend on the other: we will assume memory safety while considering control flow safety and vice versa. For each our approach will be to first establish some general constraints, and then examine each bytecode instruction to determine what additional checks are necessary to guarantee safety.

Many of our checks will depend on the \emph{method header}. Each method in our VM has a small header defining properties such as the maximum stack size, number of local variables, return type, etc. Since the VM uses this header to determine things like the required size of the stack frame, and the effects of method calls, many of the necessary checks are to ensure the actual bytecode follows the contract established in the method header.

When the node receives code, it will first receive the headers for all methods, followed by the implementation, so the contracts for all methods are known when we start translating the byte code. The first check we do is a basic sanity check on the data in the method headers (\ref{chk-method-header-is-sane}). For example, since each parameter becomes a local variable, the total number of local variable slots must be at least as high as the number of parameter slots.

%TODO: implement this as well, just to be complete

\section{Control flow safety}
To ensure control flow safety, we can group instructions into four categories. Most instructions don't affect the control flow. The ones that do are: various kinds of branches, method invocations, and returns. The state is correct at the start of the programme, since the VM will start at the beginning of the first instruction. We will show the state will be correct after each instruction by looking at these four categories.

\begin{table}[H]
\centering
\caption{Instructions affecting control flow}
\label{tbl-control-flow-instructions}
\begin{tabular}{ll}
\toprule
type     & effect on control flow \\
\midrule
branches & jump to a location within the method \\
invokes  & call a method, either through the VM or directly \\
returns  & return to the address at the top of the stack \\
others   & fall through to the next JVM instruction \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Simple instructions}
Starting with the last category: most instructions such as math operations, loads and stores, are translated to a single sequence of native instructions that will be executed top to bottom. In some cases this may \mycode{CALL} to a VM or libc function to perform some complex operation, but this is safe since these will return to the same location.

For this category, the generated code will flow naturally into the next generated instruction, which is safe as long as there \emph{is} a next instruction. This produces our second translation-time check, \ref{chk-return-or-goto-at-end-of-method}: the last instruction in a method should be a \mycode{RETURN} or \mycode{GOTO} to prevent control from flowing into undefined territory after the method body.

\subsection{Branch instructions}
In our bytecode, branches don't target a bytecode offset as in normal JVM bytecode, but a branch target ID. These targets are marked with a \mycode{BRTARGET} instruction, which doesn't emit any code, but causes the AOT compiler to collect the address in a temporary table during translation. Once the whole method is translated, this temporary table is used to patch the correct target address into the branch instructions to handle forward branches.

This makes checking the branch addresses easy. Each method announces the number of branch targets that will be used in the method header. To ensure each taken branch will branch to a legal instruction within the method, we need to check that the target ID of each branch is lower than the number of branch targets announced (\ref{chk-brtarget-exists}) so it points to an entry within the table, and that at the end of the method, all \mycode{BRTARGET} instructions have been found so all entries in the table point to the start of a translated instruction (\ref{chk-all-brtargets-found}).

\subsection{Invoke instructions}
We have three kinds of method invocations in our VM.

For lightweight method calls, we require the target to be declared before any call is made, to ensure the address of target is known at translation time and we can directly generate a \mycode{CALL} to it. Ensuring this calls to a correct address is therefore trivial: we must simply check the code for the method has already been generated (\ref{chk-invokelight-target-found}).

For static calls (\mycode{INVOKESTATIC} and \mycode{INVOKESPECIAL}) the target method is known at translation time, but it may not have been translated yet if the implementation follows later in the infusion. For these instructions, we generate a \mycode{CALL} to the VM's \mycode{callMethod} function, and pass it the id of the target method. At run time this id will be used as an index in the method table. Since we will store all the method headers before translating the implementations, we can check at translation time the method id is known (\ref{chk-invokestatic-target-header-found}), and since the VM won't start an application before the implementation for all methods is translated, this guarantees that \mycode{callMethod} will find the correct target at runtime.

Finally, for \mycode{INVOKEVIRTUAL} and \mycode{INVOKEINTERFACE}, we do not know the target at translation time, since \mycode{callMethod} will resolve this depending on the object we will invoke the method on, which will not be known at runtime. Therefore we need a run-time check to ensure the method can be found (\ref{chk-invokevirtual-target-found}), but this doesn't add any overhead since the method must be resolved to make the call anyway.

\subsection{Return instructions}
Finally, return instructions will pop the return value from the stack, and then do a native \mycode{RET} instruction to return, either directly the AOT compiled code of the caller for lightweight methods, or to the VM's \mycode{callMethod} for other methods.

The \mycode{RET} instruction will take the return address from the native stack, which is also used to store the JVM's integer operand stack. This means we need to check the integer operand stack is empty at return instructions (\ref{chk-stack-is-empty-after-return}), to ensure the return address will be at the top of the native stack. Memory safety then guarantees the application could not have corrupted it.

The second way the return address could be corrupted is if the native stack overflows into the heap. In our VM the heap is a fixed sized array that sits above other global variables, and the AVR's native stack grows down towards it. If the native stack were to grow into the area reserved for the heap, a return address may be corrupted.

We add a check during non-lightweight invokes that the stack frame for the called method, plus it's maximum integer stack size, does not grow into the heap. (\ref{chk-no-nativestack-overflow})

Lightweight calls do not add to the stack, since space for their local variables, stack, and return address was already allocated in the caller's frame. However, a method may make calls to the VM or libc functions. We can determine the maximum stack growth for all the possible calls our AOT generated code may make. If this were to grow into the heap, this could corrupt the stack. Therefore we add a certain safety margin between the stack and heap, which we have currently set conservatively, but could be reduced by adding extra stack checks in vm functions that cause the most stack growth.
%TODO: find reference about max stack size

\section{Memory safety}
\begin{figure}[]
  \includegraphics[width=\linewidth]{memlayout.eps}
  \caption{Global memory layout}
  \label{fig-memlayout}
\end{figure}

Figure \ref{fig-memlayout} shows the global layout of the VM's memory. At the bottom are the internal state of by the VM, stored in a number of global variables, and a block with static variables for each infusion. The infusion blocks are actually allocated on the heap at application startup time, but this part of the heap is then separated from the range the application may use to protect them from bad heap writes. This is followed by the application heap, which containts Java objects and arrays.

The native stack grows down in memory towards the heap. This contains a mix of native stack frames for internal VM functions, the application's JVM stack frames containing space for local variables and reference operand stack, and the integer operand stack which grows down directly on the native stack.

This shows how the VM's private data and the application data are mixed in the node's memory. The application is only allowed to write to the areas indicated with bars to the right. Any write outside of these designated areas may corrupt the VM's internal state, and needs to be prevented by our safety checks.

Having ensured control flow safety, we can now rely on the fact that we will always execute complete JVM instructions, and the application cannot skip past an inserted run-time check by jumping to the middle of a generated instruction. This means we can demonstrate memory safety by considering each how each JVM instruction writes to memory, and showing they are all either guaranteed to write to a correct address, or checked at run-time. All JVM instructions can be grouped into one of the categories shown below with respect to their writes to memory.

\begin{table}[H]
\centering
\caption{Instructions writing to memory}
\label{tbl-control-flow-instructions}
\begin{tabular}{ll}
\toprule
type     & writes to \\
\midrule
\mycode{STORE}                   & local variable in stack frame \\
\mycode{PUTSTATIC}               & static variable in infusion \\
\mycode{PUTARRAY}                & heap \\
\mycode{PUTFIELD}                & heap \\
any instruction pushing          & reference or integer stack \\
~~to the stack                   & \\
\bottomrule
\end{tabular}
\end{table}


Since the JVM stack frames are created by the VM based on the method header, we know how much space will be available for local variables and the references stack for normal methods. For lightweight methods, we depend on the caller's stack frame, so we need to make sure this is large enough. Whenever we translate an \mycode{INVOKELIGHT} instruction we verify the stack frame of the current method has reserved enough free space for the lightweight method's locals and stack (\ref{chk-sufficient-stack-space-at-invokelight} and \ref{chk-sufficient-locals-at-invokelight}).

\subsection{The operand stack}
The VM will reserve space for the operand stacks based on the information in the method header, so we need to make sure the actual stack depth neither underflows, or exceeds the maximum announced in the header.

The effect of each instruction on the stack is known at translation time. We simply the verification process by requiring the stack to be empty at all branches, so we can determine the stack depth in a single top to bottom pass. Alternatively, we could announce the expected stack state for each branch target in the method header so we can check the state matches at each branch and corresponding branchtarget. However, this is more complex and in practice the overhead for requiring an empty operand stack at branches is minimal: in all our Java code, only four trivial rewrites to avoid the \mycode{? :} operator were necessary.

This allows us to verify the stack simply by maintaining two counters indicating how many values are on the integer and reference stacks, and updating these counters for each instruction's stack effects as we translate the method. For normal methods both counters are initialised to 0, since they start with empty stacks. Lightweight methods start with their parameters on the stack, so when translating these, the counters are initialised according to the number of arguments announced in the method header.

For each translated instruction, we then check there are enough values on the stack to consume its operands (\ref{chk-no-operandstack-underflow}), and we do not exceed the maximum stack depth announced in the header after pushing it's results (\ref{chk-no-operandstack-overflow}).

Most bytecode instructions have a fixed effect on the stack, for example \mycode{IADD} will always consume two 32-bit ints and push another. We encoded this in a simple table. Method calls, discussed below, require some more work to determine the stack effects. 

\subsubsection{Invoke instructions}
The \mycode{INVOKESTATIC}, \mycode{INVOKESPECIAL}, and \mycode{INVOKELIGHT} instructions all contain the id of the method that will be invoked. We have the method headers for all methods available at translation time, so it is easy to determine the stack effects, since the number of arguments and return value are contained in the header.

For \mycode{INVOKEVIRTUAL} and \mycode{INVOKEINTERFACE} however, the actual method that will be called depends on the object on the stack at run-time. We solve this by choosing the first method implementation that matches the call, and use this method header to determine the stack effects. For valid code all implementations should have the same signature, but malicious code could try to send an implementation in a subclass that has different stack effects. We therefore add a run-time check (\ref{chk-invokevirtual-stack-effects-match}) that will verify the method called at runtime has the same stack effects as the one used to verify the stack at translation time.

\subsubsection{Return}
Note that \mycode{RETURN} instructions don't need any special care. The stack depth will be verified using the instruction found in the bytecode. The return value is passed in registers, so if the return instruction doesn't match the return type in the method header, the result is that either a return value is discarded, or whatever happens to be in registers is used as a return value.

So even though it's possible for a method to break the contract established in the method header, for example by using \mycode{RETURN} instead of \mycode{IRETURN} in a method that should return an int, this can only corrupt the application's own state, and not the VM's.

\subsection{\mycode{STORE}}
Local variable access is easy to verify at translation time. The method header is used to create the current method's stack frame, or for lightweight methods to verify any caller has reserved enough slots for the lightweight method's locals.

Local methods are accessed as an offset from the Y register, which points to the start of the local variables. Since the Y register is controlled by the VM, we only need to check at translation time that the index of the local is within the range announced in the method header to make sure we will write to a valid location (\ref{chk-local-variable-slot-exists}).

\subsection{\mycode{PUTSTATIC}}
Static are allocated globally at the start of the application based on number of static variables in the \emph{infusion} header. The \mycode{PUTSTATIC} instruction contains a reference to an infusion, and the index of the static variable slot. At translation time, we simply need to check the referenced infusion exists (\ref{chk-static-variable-infusion-exists}), and the index is within the legal range (\ref{chk-static-variable-slot-exists}).

\subsection{\mycode{PUTFIELD} and \mycode{PUTARRAY}}
The final type of memory access is to the heap, which happens using the \mycode{PUTFIELD} and \mycode{PUTARRAY} instructions to write to object fields and array elements respectively. The various \mycode{NEW} instructions used to create them are assumed to be safe since they are fully implemented in the VM.

Access to objects and arrays is hard to verify at translation time, without extensive analysis that would be too expensive for a sensor node. For example, a null pointer bug could easily trick the VM to write to the lowest addresses. Since in the AVR, the lowest 32 bytes of the address space are mapped to the CPU's general purpose registers, this can cause very hard to diagnose bugs, as we have experienced first-hand. Similarly, using a high out-of-bounds index into an array, malicious code could easily gain access to the native stack and, for instance, corrupt return addresses.

Here we add a run-time check when translating the instructions just before doing the final memory access to check the address is within the heap (\ref{chk-memory-access-within-heap}).

The VM will set the bounds of the heap in two variables: \mycode{heap\_lo\_bound} and \mycode{heap\_hi\_bound} as shown in Figure \ref{fig-memlayout}. The address to write to is calculated in the AVR's \mycode{Z} register for each heap access instruction. Just before we write to the heap, we insert al \mycode{CALL} to the \mycode{heapcheck} function shown in Listing \ref{lst-heap-bounds-check}.

This function checks the address in \mycode{Z} is within these bounds. If it is not, it will jump to the \mycode{illegal\_access\_handler}, allowing the VM to terminate the application. This check will add 22 cycles overhead for each memory access, and 4 bytes code size overhead for the \mycode{CALL} instruction.

The actual write to the heap is often done by an offset from Z. For example if the offset for an object field is known at compile time, Z is simply loaded with the object's address and the access is done using the correct offset if is less than 64 bytes. If it is higher, Z is first increased to bring it into range.

This means the write could end up at most 63 bytes above the end of the heap, for which we reuse the same small safety margin mentioned in check \ref{chk-no-nativestack-overflow}, which is safe since we will never be writing to a heap object and executing a function in the VM or libc at the same time.

\subsubsection{Alternatives}
We considered several alternative implementations. Since the \mycode{CALL} and \mycode{RET} instruction account for 8 out of these 22 cycles, the checks could be made faster by inlining them. However, inlining would increase the code size overhead from 4 bytes to over 30 bytes, which we considered too high.

We can also save 8 cycles if we keep the bounds in registers instead of memory, which would remove the need for the \mycode{LDS} instructions. However this reduces the performance of the stack cache since it means we have 2 register pairs less for stack caching.

Finally, we could eliminate the need to bounds check the lower byte of the \mycode{Z} register, \mycode{ZL}, if we align the top and bottom of the heap at 256 bytes. This would save 6 cycles, but wastes on average 256 bytes of RAM since some space at the top and bottom of the heap cannot be used.

\begin{listing}[H]
	\centering
 	\begin{minted}{c-objdump}
    heapcheck:
        lds  r0, heap_lo_bound
        cp   ZL, r0
        lds  r0, heap_lo_bound + 1
        cpc  ZH, r0
        brlo illegal_access_handler:
        lds  r0, heap_hi_bound
        cp   r0, ZL
        lds  r0, heap_hi_bound + 1
        cpc  r0, ZH
        brlo illegal_access_handler:
        ret
	\end{minted}
	\caption{Heap bounds check}
	\label{lst-heap-bounds-check}
\end{listing}

%\begin{listing}[H]
%	\centering
% 	\begin{minted}{c-objdump}
%    heapcheck:                               cycles   bytes
%        lds  r0, heap_lo_bound               2        4
%        cp   ZL, r0                          1        2
%        lds  r0, heap_lo_bound + 1           2        4
%        cpc  ZH, r0                          1        2
%        brlo illegal_access_handler:         1        2
%        lds  r0, heap_hi_bound               2        4
%        cp   r0, ZL                          1        2
%        lds  r0, heap_hi_bound + 1           2        4
%        cpc  r0, ZH                          1        2
%        brlo illegal_access_handler:         1        2
%        ret                                  4        2
%	\end{minted}
%	\caption{Heap bounds check}
%	\label{lst-heap-bounds-check}
%\end{listing}