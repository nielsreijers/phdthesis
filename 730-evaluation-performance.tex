\input{730-tbl-evaluation-performance}


\section{AOT translation: performance}
\label{sec-evaluation-aot-translation-performance}

Next we will look at the effect of our optimisations to the baseline AOT translation approach, for all our benchmarks. The trace data produced by Avrora gives us a detailed view into the run-time performance and the different types of overhead. We count the number of bytes and cycles spent on each native instruction for both the native C and our AOT compiled version, and group them into 4 categories that roughly match the types of AOT translation overhead discussed in Section \ref{sec-overhead-aot-translation}:
\begin{itemize}
	\item \mycode{PUSH},\mycode{POP}: Matches the type 1 push/pop overhead.
	\item \mycode{LD},\mycode{LDD},\mycode{ST},\mycode{STD}: Matches the type 2 load/store overhead and directly shows the amount of memory traffic.
	\item \mycode{MOV},\mycode{MOVW}: For moves the picture is less clear since the AOT compiler emits them for various reasons. Without stack caching, it emits moves to replace push/pop pairs, and after adding the mark loops optimisation to save a pinned value when it is popped destructively.
	\item others: the total overhead, minus the previous three categories. This roughly matches the type 3 overhead.
\end{itemize}

We define the overhead from each category as the number of bytes or cycles spent in the AOT version, minus the number spent in the native version for that category, and again normalise this to the \emph{total} number of bytes or cycles spent in the native C version. The detailed results for each benchmark and for each type of overhead are shown in tables \ref{tbl-performance-per-benchmark} and \ref{tbl-codesize-per-benchmark}. In addition, Table \ref{tbl-performance-per-benchmark} also lists the time spent in the VM on method calls and allocating objects. The constant array optimisation is already included in these results, since MoteTrack cannot run without it. We will examine its effect separately in section \ref{sec-evaluation-const-array}.

Table \ref{tbl-performance-per-benchmark} first shows the results without any optimisation to either the AOT compilation process or the original, direct translation of the C source code to Java. This results in a large overhead of up to 20x slowdown for \mybench{heap sort}, which in this case is mostly due to method calls since small functions and macros in the C code are not inlined in this version. Optimising the source code reduces overhead dramatically, but this is partly because the other optimisations, which target some of the same overhead, have not yet been applied. For example, in Table \ref{tbl-performance-per-benchmark} optimising the source code reduces \mybench{CoreMark}'s overhead by 434\%, while the previous section showed that when all other optimisations are applied first, the difference is only 268\%. Since the source code optimisations were discussed in the previous section, the rest of this evaluation will focus on the effect of the other optimisations on the already optimised source.

Figure \ref{fig-performance-per-opcode-category} starts with the manually optimised source code and incrementally adds each optimisation to the AOT compiler to show how they combine to reduce performance overhead. We take the average of all benchmarks, and show both the total overhead, and the overhead for each instruction category. Figure \ref{fig-performance-per-benchmark} shows the total overhead for each individual benchmark.

\begin{figure}
\centering
\includegraphics[width=\mygraphsize]{performance-per-opcode-category3a.eps}
\caption{Performance overhead per category}
\label{fig-performance-per-opcode-category}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\mygraphsize]{performance-per-benchmark3a.eps}
\caption{Performance overhead per benchmark}
\label{fig-performance-per-benchmark}
\end{figure}

Using the baseline AOT compilation on the optimised sources, the types 1, 2 and 3 overhead are all significant, at 138\%,  109\%, and 73\% respectively, and the 50\% overhead in the VM is mainly spent on method calls since the overhead from allocating temporary objects is already removed by the source code optimisations. The basic approach does not have many reasons to emit a move, so in some cases the AOT version actually spends fewer cycles on move instructions than the C version, resulting in small negative values.  When we improve the peephole optimiser to include non-consecutive push/pop pairs, push/pop overhead drops by 100.2\% (of native C performance), but if the push and pop target different registers, they are replaced by a move instruction, and we see an increase of 11.5\% in move overhead. For a 16-bit register pair this takes 1 cycle (for a \mycode{MOVW} instruction), instead of 8 cycles for two pushes and two pops. The increase in moves shows most of the extra cases that are handled by the improved peephole optimiser are replaced by a move instead of eliminated, since the 11.5\% extra move overhead matches a 92\% reduction in push/pop overhead.

Next stack caching is introduced to utilise all available registers and eliminate most of the push/pop instructions that cannot be handled by the peephole optimiser. As a result the push/pop overhead drops to nearly 0, and so does the move overhead since most of the moves introduced by the peephole optimiser, are also unnecessary when using stack caching.

Having eliminated the type 1 overhead almost completely, popped value caching is added to remove a large number of the unnecessary load instructions. This reduces the memory traffic significantly, as is clear from the reduced load/store overhead, while the other types remain stable. Adding the mark loops optimisation further reduces loads, and this time also stores, by pinning common variables to a register. But it uses slightly more move instructions, and the fact that fewer registers are available for stack caching means stack values are spilled to memory more often. While 53.0\% is saved on loads and stores, the push/pop and move overhead increase by 6.0\% and 5.6\% respectively.

Most of the push/pop and load/store overhead has now been eliminated and the type 3 overhead, unaffected by these optimisations, has become the most significant source of overhead. This type has many different causes, and only part of it can be eliminated with the instruction set optimisations. These optimisations, especially the 16-bit array index, also reduce register pressure, which results in a slight decrease in the other overhead types, although this is minimal in comparison. The \mybench{CoreMark} and \mybench{FFT} benchmarks are the only ones to do 16-bit to 32-bit multiplication, so the average performance improvement for \mycode{SIMUL} is small, but Table \ref{tbl-performance-per-benchmark} shows it is significant for these two benchmarks.

Finally, the lightweight optimisation could be applied to almost every method. Lightweight methods still incur some overhead, which will be discussed in more detail in Section \ref{sec-evaluation-method-invocation}, but since they do not call the VM, the time spent in the VM on method calls is effectively eliminated.

Combined, the optimisations to the AOT compilation process reduce performance overhead from 377\% to 67\% of native C performance.
