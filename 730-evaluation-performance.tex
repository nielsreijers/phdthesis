\input{730-tbl-evaluation-performance}


\section{AOT translation: performance}
\label{sec-evaluation-aot-translation-performance}

Next we will look at the effect of our optimisations to the baseline AOT translation approach, for all our benchmarks.

The trace data produced by Avrora gives us a detailed view into the run-time performance and the different types of overhead. We count the number of bytes and cycles spent on each native instruction for both the native C and our AOT compiled version, and group them into 4 categories that roughly match the types of AOT translation overhead discussed in Section \ref{sec-overhead-aot-translation}:
\begin{itemize}
	\item \mycode{PUSH},\mycode{POP}: Matches the type 1 push/pop overhead since native code uses almost no push/pop instructions.
	\item \mycode{LD},\mycode{LDD},\mycode{ST},\mycode{STD}: Matches the type 2 load/store overhead and directly shows the amount of memory traffic.
	\item \mycode{MOV},\mycode{MOVW}: For moves the picture is less clear since the AOT compiler emits them for various reasons. Before we introduce stack caching, it emits moves to replace push/pop pairs, and after the mark loops to save a pinned value when it is popped destructively.
	\item others: the total overhead, minus the previous three categories. This roughly matches the type 3 overhead.
\end{itemize}

We define the overhead from each category as the number of bytes or cycles spent in the AOT version, minus the number spent by the native version for that category, and again normalise this to the \emph{total} number of bytes or cycles spent in the native C version. The detailed results for each benchmark and type of overhead are shown in tables \ref{tbl-performance-per-benchmark} and \ref{tbl-codesize-per-benchmark}. In addition, Table \ref{tbl-performance-per-benchmark} also lists the time spent in the VM on method calls.

In Figure \ref{fig-performance-per-opcode-category} we see how our optimisations combine to reduce performance overhead. We take the average of all benchmarks, and show both the total overhead, and the overhead for each instruction category. Figure \ref{fig-performance-per-benchmark} shows the total overhead for each individual benchmark. We start with the original AOT approach with only the simple peephole optimiser, and then incrementally add each of our optimisations. The lightweight method call optimisation is already included in these results. We will examine its effect separately in Section \ref{sec-evaluation-method-invocation}.

\begin{figure}
\centering
\includegraphics[width=\mygraphsize]{performance-per-opcode-category.eps}
\caption{Performance overhead per category}
\label{fig-performance-per-opcode-category}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\mygraphsize]{performance-per-benchmark.eps}
\caption{Performance overhead per benchmark}
\label{fig-performance-per-benchmark}
\end{figure}

Using the simple optimiser, the types 1, 2 and 3 overhead are all significant, at 136\%,  107\%, and 70\% respectively. The basic approach does not have many reasons to emit a move, so we see that in some cases the AOT version actually spends fewer cycles on move instructions than the C version, resulting in small negative values.  When we improve the peephole optimiser to include non-consecutive push/pop pairs, push/pop overhead drops by 99.4\% (of native C performance), but if the push and pop target different registers, they are replaced by a move instruction, and we see an increase of 11.5\% in move overhead. For a 16-bit value this takes 1 cycle (for a MOVW instruction), instead of 8 cycles for two pushes and two pops. The increase in moves shows most of the extra cases that are handled by the improved optimiser are replaced by a move instead of eliminated, since the 11.5\% extra move overhead matches a 92\% reduction in push/pop overhead.

Next we introduce stack caching to utilise all available registers and eliminate most of the push/pop instructions that cannot be handled by the improved optimiser. As a result the push/pop overhead drops to nearly 0, and so does the move overhead since most of the moves introduced by the peephole optimiser, are also unnecessary when using stack caching.

Having eliminated the type 1 overhead almost completely, we now add popped value caching to remove a large number of the unnecessary load instructions. This reduces the memory traffic significantly, as is clear from the reduced load/store overhead, while the other types remain stable. Adding the mark loops optimisation further reduces loads, and this time also stores, by pinning common variables to a register. But it uses slightly more move instructions, and the fact that we have fewer registers available for stack caching means we have to spill stack values to memory more often. While we save 50.8\% on loads and stores, the push/pop and move overhead increase by 4.5\% and 5.7\% respectively.

Most of the push/pop and load/store overhead has now been eliminated and the type 3 overhead, unaffected by these optimisations, has become the most significant source of overhead. This type has many different causes, but we can eliminate part of it with our three instruction set optimisations. These optimisations, especially the 16-bit array index, also reduce register pressure, so we also see slight decreases in the other overhead types, although this is minimal in comparison. The CoreMark and FFT benchmarks are the only ones to do 16-bit to 32-bit multiplication, so the average performance improvement for \mycode{SIMUL} is small, but Table \ref{tbl-performance-per-benchmark} shows it is significant for these two benchmarks.

Combined, these optimisations reduce performance overhead from 320\% to 71\% of native C performance.
