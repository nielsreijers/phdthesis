\section{Code size}
\begin{table}[]
 \centering
 \caption{Code size data per benchmark}
 \label{tbl-codesize-per-benchmark}
 \small
 \scriptsize
 \setlength{\tabcolsep}{4pt}
 \input{740-tbl-evaluation-code-size}
 \setlength{\tabcolsep}{6pt}
\end{table}

%TODO: say something about the size of class initialisers for constant data tables (FFT8,FFT16,MoteTrack)

Next we examine the effects of our optimisations on code size. Two factors are important here: the size of the VM itself and the size of the code it generates.

The size overhead for the generated code is shown in figures \ref{fig-codesize-per-opcode-category} and \ref{fig-codesize-per-benchmark}, again split up per instruction category and benchmark respectively. For the first three optimisations, the two graphs follow a similar pattern as the performance graphs. These optimisations eliminate the need to emit certain instructions, which reduces code size and improves performance at the same time.

The mark loops optimisation moves loads and stores for pinned variables outside of the loop. This reduces performance overhead by 34\%, but the effect on code size varies per benchmark: some are slightly smaller, others slightly larger.

For each value that is live at the beginning of the loop, we need to emit the load before the mark loop block, so in terms of code size we only benefit if it is loaded more than once, and may actually lose some if it is then popped destructively, since we would need to emit a \mycode{mov}. Stores follow a similar argument. Also, for small methods the extra registers used may mean we have to save more call-saved registers in the method prologue. Finally, we get the performance advantage for each run-time iteration, but the effect on code size, whether positive or negative, only once.

\begin{figure}
 \centering
 \begin{minipage}{0.45\textwidth}
  \centering
  \includegraphics[width=\myfiguresizecodesize]{codesizeoverhead-per-opcode-category.eps}
  \caption{Code size overhead per category}
  \label{fig-codesize-per-opcode-category}
 \end{minipage}\hfill
 \begin{minipage}{0.45\textwidth}
  \centering
  \includegraphics[width=\myfiguresizecodesize]{codesizeoverhead-per-benchmark.eps}
  \caption{Code size overhead per benchmark}
  \label{fig-codesize-per-benchmark}
 \end{minipage}
\end{figure}

The constant shift optimisation unrolls the loop that is normally generated for bit shifts. This significantly improves performance, but the effect on the code size depends on the number of bits to shift by. The constant load and loop take at least 5 instructions. In most cases the unrolled shifts will be smaller, but md5 actually shows a small 4\% increase in code size since it contains many shifts by a large number of bits.

Using 16-bit array indexes also reduces code size. The benchmarks here already have the manual code optimisations, so they use short index variables. This means the infuser will emit a \mycode{S2I} instruction to cast them to 32-bit ints if the array access instructions expect an int index. Not having to emit those when the array access instructions expect a 16-bit index, and the reduced work the access instruction needs to do, saves about 13\% code size overhead in addition to the 23\% reduction in performance overhead. Using 32-bit variables in the source code would also remove the need for \mycode{S2I} instructions, but the extra code to manipulate the index variable would make the net code size even larger.

\subsection{VM code size and break-even point}
These more complex code generation techniques do increase the size of our compiler. The first column in Table \ref{tbl-code-size-and-memory-consumption} shows the difference in code size between the AOT translator and Darjeeling's interpreter. The basic AOT approach is 6245B larger than the interpreter, and each of our optimisations adds a little to the size of the VM.

They also generate significantly smaller code. The second column shows the reduction in the generated code size compared to the baseline approach. Here we show the reduction in total size, as opposed to the overhead used elsewhere, to be able to calculate the break-even point. Using the improved peephole optimiser adds 278 bytes to the VM, but it reduces the size of the generated code by 14.4\%. If we have more than 1.9KB available to store user programmes, this reduction will outweigh the increase in VM size. Adding more complex optimisations further increases the VM size, but compared to the baseline approach, the break-even point is well within the range of memory typically available on a sensor node, peaking at at most 18.1KB.

As is often the case, there is a tradeoff between size and performance. The interpreter is smaller than each version of our AOT compiler, and Table \ref{tbl-codesize-per-benchmark} shows JVM bytecode is smaller than both native C and AOT compiled code, but the interpreter's performance penalty may be unacceptable in many cases. Using AOT compilation we can achieve adequate performance, but the most important drawback has been an increase in generated code size. These optimisations help to mitigate this drawback, and both improve performance, and allow us to load more code on a device.

For the smallest devices, or if we want to be able to load especially large programmes, we may decide to use only a selection of optimisations to limit the VM size and still get both a reasonable performance, and most of the code size reduction. Using only popped value stack caching reduces code size by 33.3\%, and results in a performance overhead of 156\%. The  16-bit array index optimisation should also be included, since this reduces the size of both the VM and the generated code.

\begin{table*}[]
% Memory consumption is based on using only 11 element arrays. Current implementation is lazy and just uses a 16 byte array, but we never use the first two and last three, since they're not used for stack caching. Original:
%baseline             &  20B &  4450B &          &       \\
%improved peephole    &  20B &  5016B &  -13.1\% &  4.3KB  \\
%simple               &  36B &  6054B &  -28.1\% &  5.7KB  \\
%popped               & 104B &  7314B &  -34.4\% &  8.3KB  \\
%markloop             & 111B & 10172B &  -36.4\% & 15.7KB  \\
%const shift          & 112B & 10766B &  -37.2\% & 17.0KB  \\

\centering
\caption{Code size and memory consumption}
\label{tbl-code-size-and-memory-consumption}
\small
\begin{tabular}{lrrrrrr}
\toprule
                      & size vs     & size vs  &            & AOT code  & break   & memory \\
                      & interpreter & baseline &            & reduction & even    & usage  \\
\hline
Baseline              &     6245 B  &          &            &           &         & 30 B   \\
Improved peephole     &     6523 B  &    278 B &  \scriptsize (+278)  &  -14.4\%  &  1.9 KB & 30 B   \\
Simple stack caching  &     7243 B  &    998 B &  \scriptsize (+720)  &  -28.6\%  &  3.5 KB & 41 B   \\
Popped value caching  &     8607 B  &   2362 B & \scriptsize (+1364)  &  -33.3\%  &  7.1 KB & 89 B   \\
Markloop              &    11903 B  &   5658 B & \scriptsize (+3296)  &  -33.2\%  & 17.0 KB & 98 B   \\
Const shift           &    12373 B  &   6128 B &  \scriptsize (+470)  &  -33.8\%  & 18.1 KB & 98 B   \\
16-bit array index    &    12353 B  &   6108 B &   \scriptsize (-20)  &  -38.0\%  & 16.1 KB & 98 B   \\
SIMUL                 &    12419 B  &   6174 B &   \scriptsize (+66)  &  -38.1\%  & 16.2 KB & 98 B   \\
Lightweight methods   &    12961 B  &   6716 B &  \scriptsize (+542)  &  -38.4\%  & 17.5 KB & 98 B   \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{VM memory consumption} The last column in Table \ref{tbl-code-size-and-memory-consumption} shows the size of the main data structure that needs to be kept in memory while translating a method. For the baseline approach we only use 30 bytes for a number of commonly used values such as a pointer to the next instruction to be compiled, the number of instructions in the method, etc. The simple stack caching approach adds a 11 byte array to store the state of each register pair we use for stack caching. Popped value caching adds two more arrays of 16-bit elements to store the value tag and age of each value. Mark loops only needs an extra 16-bit word to mark which registers are pinned, and a few other variables. Finally, the instruction set optimisations do not require any additional memory. In total, our compiler requires 98 bytes of memory during the compilation process.


