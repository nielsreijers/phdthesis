\input{740-tbl-evaluation-code-size}

\section{AOT translation: code size}
\label{sec-evaluation-aot-translation-code-size}
Next we examine the effects of our optimisations on code size. Two factors are important here: the size of the VM itself and the size of the code it generates.

The size overhead for the generated code is shown in figures \ref{fig-codesize-per-opcode-category} and \ref{fig-codesize-per-benchmark}, again split up per instruction category and benchmark respectively. For the first three optimisations, the two graphs follow a similar pattern as the performance graphs. These optimisations eliminate the need to emit certain instructions, which reduces code size and improves performance at the same time.

The mark loops optimisation moves loads and stores for pinned variables outside of the loop. This reduces performance overhead by 41\%, but the effect on code size varies per benchmark: some are slightly smaller, others slightly larger.

For each value that is live at the beginning of the loop, we need to emit the load before the mark loop block, so in terms of code size we only benefit if it is loaded more than once, and may actually lose some if it is then popped destructively, since we would need to emit a \mycode{mov}. Stores follow a similar argument. Also, for small methods the extra registers used may mean we have to save more call-saved registers in the method prologue. Finally, we get the performance advantage for each run-time iteration, but the effect on code size, whether positive or negative, only once.

% \begin{figure}
%  \centering
%  \begin{minipage}{0.45\textwidth}
%   \centering
%   \includegraphics[width=\mygraphsize]{codesizeoverhead-per-opcode-category.eps}
%   \caption{Code size overhead per category}
%   \label{fig-codesize-per-opcode-category}
%  \end{minipage}\hfill
%  \begin{minipage}{0.45\textwidth}
%   \centering
%   \includegraphics[width=\mygraphsize]{codesizeoverhead-per-benchmark.eps}
%   \caption{Code size overhead per benchmark}
%   \label{fig-codesize-per-benchmark}
%  \end{minipage}
% \end{figure}

\begin{figure}
\centering
\includegraphics[width=\mygraphsize]{codesizeoverhead-per-opcode-category.eps}
\caption{Code size overhead per category}
\label{fig-codesize-per-opcode-category}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\mygraphsize]{codesizeoverhead-per-benchmark.eps}
\caption{Code size overhead per benchmark}
\label{fig-codesize-per-benchmark}
\end{figure}

The constant shift optimisation unrolls the loop that is normally generated for bit shifts. This significantly improves performance, but the effect on the code size depends on the number of bits to shift by. The constant load and loop take at least 5 instructions. In most cases the unrolled shifts will be smaller, but \mybench{MD5} and \mybench{XXTEA} show a small increase in code size since they contains shifts by a large number of bits.

Using 16-bit array indexes also reduces code size. The benchmarks here already have the manual code optimisations, so they use short index variables. This means the infuser will emit a \mycode{S2I} instruction to cast them to 32-bit ints if the array access instructions expect an int index. Not having to emit those when the array access instructions expect a 16-bit index, and the reduced work the access instruction needs to do, saves about 13\% code size overhead in addition to the 17\% reduction in performance overhead. Using 32-bit variables in the source code also removes the need for \mycode{S2I} instructions, but the extra effort needed to manipulate the 32-bit index variable makes the net code size even larger.

\subsection{VM code size and break-even point}
These more complex code generation techniques do increase the size of our compiler. The first column in Table \ref{tbl-code-size-and-memory-consumption} shows the difference in code size between the AOT translator and Darjeeling's interpreter. The basic AOT approach is 6605B larger than the interpreter, and each of our optimisations adds a little to the size of the VM.

They also generate significantly smaller code. The third column shows the reduction in the generated code size compared to the baseline approach. Here we show the reduction in total size, as opposed to the overhead used elsewhere, to be able to calculate the break-even point. Using the improved peephole optimiser adds 276 bytes to the VM, but it reduces the size of the generated code by 14.8\%. If we have more than 1.9KB available to store user programmes, this reduction will outweigh the increase in VM size. Adding more complex optimisations further increases the VM size, but compared to the baseline approach, the break-even point is well within the range of memory typically available on a sensor node, peaking at at most 18.3KB.

As is often the case, there is a tradeoff between size and performance. The interpreter is smaller than each version of our AOT compiler, and Table \ref{tbl-codesize-per-benchmark} shows JVM bytecode is smaller than both native C and AOT compiled code, but the interpreter's performance penalty may be unacceptable in many cases. Using AOT compilation we can achieve adequate performance, but the most important drawback has been an increase in generated code size. These optimisations help to mitigate this drawback, and both improve performance, and allow us to load more code on a node.

For the smallest devices, or if we want to be able to load especially large programmes, we may decide to use only a selection of optimisations to limit the VM size and still get both a reasonable performance, and most of the code size reduction. For example, dropping the markloop and constant shift optimisations would reduce the size of the VM by 3.5KB while maintaining most of the reduction in generated code size, while performance overhead would increase to about 126\%.

\input{741-tbl-evaluation-vm-size}

\subsection{VM memory consumption} The last column in Table \ref{tbl-code-size-and-memory-consumption} shows the amount of data that needs to be kept in memory while translating a method. We would like our VM to be able to load new code while other tasks are running concurrently. Here we only list the data that the VM would need to maintain in between receiving messages with new code over the air since this indicates the amount of memory that needs to be reserved and would not be available to other tasks during this process. Of course while a message with new code is being processed, more stack memory is used, but this is freed again after a message has been processed and can be reused by other applications.

For the baseline approach we only use 23 bytes for a number of commonly used values such as a pointer to the next instruction to be compiled, the number of instructions in the method, etc. The simple stack caching approach adds a 11 byte array to store the state of each register pair we use for stack caching. Popped value caching adds two more arrays of 16-bit elements to store the value tag and age of each value. Mark loops only needs an extra 16-bit word to mark which registers are pinned, and a few other variables. Finally, the instruction set optimisations do not require any additional memory. In total, our compiler requires 85 bytes of memory during the compilation process.


