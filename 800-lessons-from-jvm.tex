\chapter{Lessons from JVM}
\begin{table}
    \centering
    \caption{Point requiring attention in future sensor node VMs}
    \scriptsize
    \label{tbl-issues}
    \input{800-tbl-issues}
\end{table}

\label{sec-lessons-from-jvm}
While developing MyVM, we encountered a number of situations where Java/JVM was not a good match for sensor node code. In this chapter we will discuss these.

In Section \ref{sec-introduction-scope} we defined our main research as how close an AOT compiling sensor node VM can come to native performance, and whether a VM is an efficient way to provide a safe execution environment. These questions are not specific to Java. The main motivation to base MyVM on Java was the availability of a rich set of tools and infrastructure to build on, including a solid VM to start from in the form of Darjeeling.

One aspect of Java/JVM that makes it an attractive choice for sensor nodes is its simplicity, allowing (a subset of) it to be implement in as little as 8KB \cite{Harbaum}. However, it also lacks some features that make it a less good fit for a typical sensor node code in a number of situations. These range from lack of type definitions and explicit inlining that are minor annoyances that reduce code readability, to the lack of support for constant data that could be a show stopped for a number of applications.

Many of the points we raise could be improved with minor changes to Java, leading us to a 'sensor node Java', much like nesC \cite{Gay:2003up} is a sensor node version of C, but some require more drastic changes. We believe if we were to develop a sensor node VM from scratch, with the aim of providing platform independence, safety, and performance through AOT compilation, we would end up with a design very different from JVM.

In this chapter we discuss the most pressing issues we encountered, summarised in Table \ref{tbl-issues}, and suggest ways they could be improved in future VMs. While we haven't implemented any of these changes, and the possible solutions we suggest require much more study to be turned into a working system, we believe this list will be valuable to future sensor node VM developers.




\section{A tailored standard library}
\label{sec-std-lib}
\begin{table}
    \centering
    \caption{Size of Darjeeling VM components}
    \scriptsize
    \label{tab-vm-size}
    \input{800-tbl-vm-size} 
\end{table}

A minimum Java APIs for resource constrained devices was proposed by Sun Microsystems, namely the Connected Limited Device Configuration (CLDC) specification \cite{CLDC}. CLDC was primarily intended for devices larger than typical sensor nodes, and not tailored to the characteristics of typical sensor node code. Providing support for the full CLDC specification would require a substantial amount of memory and program space for features that are rarely required for sensor node applications. Table \ref{tab-vm-size} shows the code size of library support as implemented in the Darjeeling VM as used in the WuKong project \cite{Reijers:2013ut}.

The largest mismatch comes from the CLDC's string support, which takes up over 8KB. While string support is one of the most basic features one would expect to find in the standard library of any general purpose language, it is rarely required within sensor node applications that typically don't have a UI but only communicate with the outside world through radio messages.

%TODO: include this?
% Communication with peripheral hardware is often by means of a stream of bytes. To facilitate such code it would be of benefit to support constant strings at the programming language level (which could be converted to byte arrays under the hood).

On the other hand, the standard library should include abstractions for typical sensor node operations that are missing from the CLDC. The CLDC \mycode{Stream} abstraction is intended to facilitate file, network and memory operations. The abstraction is not well suited for communication protocols required by WSN applications, such as I$^{2}$C and SPI. In CLDC, connections between devices can be initiated by specifying URI-like strings. However, processing these is relatively expensive, and WSN nodes often identify other nodes using a 16 or 32-bit identifier.

%Other CLDC features that are not required within WSN applications include dynamic class loading, calendar and timezone support. It is clear from the specification that resource constrained WSN applications were not the intended target devices.

We argue that a tailored library should be designed from the ground-up specifically for sensor node applications. Such a library would include functionality for: (i) basic math; (ii) array operations; (iii) a communication API that encapsulates the low-level protocols typically used (e.g. I$^{2}$C); and (iv) a higher-level generic radio and sensor API abstraction.




\section{Support for constant data}
\label{sec-const-data}
%\ref{sec-evaluation-benchmark-implementation-motetrack}
While Java allows us to declare variables as \mycode{final}, this is only a language level feature, and the VM has no concept of constant data. This is not surprising, since most physical CPUs do not make the distinction either. However, this is different on a sensor node's Harvard architecture where code and data memory are split. The amount of flash memory is usually several times larger than the available RAM, so constant data should be kept in flash instead of wasting precious RAM on data that will never change.

This is especially important for arrays of constant data, which are common in WSN applications. When we implement this as a \mycode{final} Java array, the compiler emits a static class initialiser that creates a Java array object, and then uses the normal array access instructions to initialise each element individually:

\begin{minted}{c}
sspush(256); newarray;                       // create the array
adup; sconst_0;    sconst_0;   bastore;      // set index 0
adup; sconst_1;    sconst_3;   bastore;      // set index 1
adup; sconst_2;    bspush(6);  bastore;      // set index 2
...
adup; bspush(255); bspush(-3); bastore;      // set index 255
\end{minted}

There are two problems with this: (i) the array will occupy scarce RAM; and (ii) initialising array elements using JVM instructions requires 4 instructions per element, resulting in 1669 bytes of code to initialise a 256 byte array.

Arrays of constant data appear in several of our benchmarks. The LEC benchmark contains two arrays of 17 16-bit and 8-bit, which are small enough to store in RAM. In this case the lack of support for constant data wastes a few bytes of memory, but doesn't prevent us from implementing the algorithm. However, in 3 other cases it did.

\paragraph{CoreMark}
The CoreMark benchmark contains a number of constant arrays with the expected outcome of the benchmark. This data is not used in the actual measured benchmark, but only to verify its results are correct. When storing this as normal Java arrays, they took up too much memory to run the benchmark, we transformed them into functions that return the correct value depending on an index passed as a parameter.

\paragraph{FFT}
The FFT benchmark contains an array of sine wave values that would be too expensive to calculate at run-time. For the 8 bit version, this contains 256 bytes, which is small enough to fit in RAM. However, the 16-bit version contained an array of 1024 16-bit values, which in itself would still fit in memory, but did not leave enough free memory run the benchmark.

In this case this was solved by storing this array in flash and adding a Java method implemented in C to read from it. This allows us to run the benchmark, but means we would only be able to run the benchmark on devices with this specific function available.

\paragraph{MoteTrack}
Finally, the MoteTrack benchmark contains a large dictionary of reference RSSI signatures that are used to determine a node's location by matching the observed RSSI values to known locations in the reference database.

At 20560 bytes, this dictionary is small enough to fit in flash memory, but over 5 times larger than the available RAM. Similar to the 16-bit FFT benchmark, the only way to implement this in a VM without support for this kind of constant data, is to store it in flash and use a native method to read the signatures.

\paragraph{Possible solutions}
Supporting constant arrays will require changes to both the language and bytecode. From the programmer's perspective it should be enough to simply add an annotation like `\mycode{progmem}' to an array declaration to tell the compiler it should be stored in flash. The bytecode will then need to be expanded to allow constant arrays in the constant pool, and to add new versions of the array load instructions (e.g. \mycode{AALOAD} and \mycode{IALOAD}) to read from them.




\section{Support for nested data structures}
\label{sec-nested-data}
Besides the need to support constant data, the MoteTrack benchmark exposes another weakness of Java: it does not support complex data structures of many small elements (anything larger than a primitive type) efficiently. Listing \ref{lst-motetrack-data-structure} shows the main \mycode{RefSignature} data structure used in MoteTrack. This structure consists of a location, which is a simple struct of 3 shorts, and a signature, which has an id, and an array of 18 signals. A signal is defined by a source ID, and an array of 2 elements with rssi values.

%TODO: consistent font size for all listings
\begin{listing}[H]
\small
\centering
\begin{minted}{c}
#define NBR_RFSIGNALS_IN_SIGNATURE 18
#define NBR_FREQCHANNELS            2

struct RefSignature
{
    Point location;
    Signature sig;
};

struct Point
{
    uint16_t x;
    uint16_t y;
    uint16_t z;
};

struct Signature
{
    uint16_t id;
    RFSignal rfSignals[NBR_RFSIGNALS_IN_SIGNATURE];
};

struct RFSignal
{
    uint16_t sourceID;
    uint8_t rssi[NBR_FREQCHANNELS];
};
\end{minted}
\caption{MoteTrack \mycode{RefSignature} data structure}
\label{lst-motetrack-data-structure}
\end{listing}

\begin{figure}[]
  \centering
  \includegraphics[width=0.9\linewidth]{motetrack-refsignature-objects}
  \caption{The \mycode{RefSignature} data structure as C struct, and collection of Java objects}
  \label{fig-motetrack-refsignature-objects}
\end{figure}

Since all the arrays are of fixed length, in C the layout of the whole structure is known at compile time, shown in Figure \ref{fig-motetrack-refsignature-objects}.

In Java, we have no concept of a struct. As described in Section \ref{sec-background-jvm-memory}, in Java every object is made up of a list of primitive values: either an int or a reference to another object. Thus, the most natural way to translate the C structures in Listing \ref{lst-motetrack-data-structure} to Java, is as a collection of objects on the heap, as shown in the right half of Figure \ref{fig-motetrack-refsignature-objects}. Note that every one of the 18 \mycode{RFSignal} structs becomes an object, which in turn has a pointer to an array of rssi values.

There are two problems with this. First, since the location of these Java objects is not known until runtime, there is a performance penalty for having to follow the chain of references. MoteTrack will loop over the signals to compare two signatures. If we start from the \mycode{rfSignals} array, Java needs to lookup the right element in \mycode{rfSignals} to get the right signal, then lookup the \mycode{rssi} field to get the RSSI array, and then another array lookup to get the right RSSI value. For the C version, all the offsets are known at compile time, so the compiler can generate a much more efficient loop, directly reading from the right offsets.

The second problem is the added memory usage. The C struct only takes up 80 bytes, all used to store data. The Java version allocates a total of 40 objects, 36 of which are spent on the \mycode{RFSignal} objects and their arrays of RSSI values. Each of these requires a heap header, which takes up 5 bytes in Darjeeling. In addition, the 19 arrays have a 3 byte header, and the Java objects and arrays contain a total of 39 references, which take up 2 bytes each. In total, the collection of Java objects takes up:

$80 + 40*5 + 19*3 + 39*2 = 415$ bytes

Combined with MoteTrack's other data structures, this became too large to fit in memory, which forced us to refactor the 2 element \mycode{rssi} array into two byte variable stored directly in \mycode{RFSignal}, as explained in Section \ref{sec-evaluation-benchmark-implementation-motetrack}. This allowed us to run the benchmark, but a RefSignature still takes up 235 bytes, and reading a rssi value still takes two lookups.

Generally, arrays of primitive types don't suffer from this problem and can be stored efficiently, but programmes containing large arrays of objects can cause significant overhead. In many cases we can work around this problem by flattening the structure, for instance in MoteTrack's case we could replace the array of \mycode{RFSignal}s with three separate arrays for \mycode{sourceID}s, \mycode{rssi\_0} and \mycode{rssi\_1}, but only at a significant cost in readability.




\section{Better language support for shorts and bytes}
\label{sec-small-datatypes}
Because RAM is scarce, 16-bit short and single byte data types are commonly used in sensor node code. The standard JVM only has 32 and 64-bit operations, and variables (instance, local or static) and stack values are stored as 32-bit, even if the actual type is shorter. On a sensor node this wastes memory, and causes a performance overhead since most nodes have 8-bit or 16-bit architectures, so many sensor node JVMs, including Darjeeling, introduce 16-bit operations and store values in 16-bit slots.

However, redesigning the VM is only half of the solution. At the language level, Java defines that an expression evaluates to 32-bits, or 64-bits if at least one operand is long. Attempting to store this in a 16-bit variable will result in a `lossy conversion' error at compile time, unless explicitly cast to a \mycode{short}.

As an example, if we have 3 short variables, a, b, and c, and want to do 
\mintinline{java}{a=b+c;}, we need to insert a cast to avoid errors from the Java compiler:

\mintinline{java}{a=(short)(b+c);}

Passing literal integer values to a method call treats them as ints, even if they are short enough to fit in a smaller type, which means we end up with calls like: 

\mintinline{java}{f((byte)1);}

While seemingly a small annoyance, in more complex code that frequently uses of shorts and bytes, these casts can make the code much harder to read.

\paragraph{Possible solutions}
If we want to have an efficient sensor node VM, both from a performance and memory usage perspective, better support for data types smaller than 32-bit integers is necessary.

We suggest that C-style automatic narrowing conversions would make most sensor node code more readable, but to leave the option of Java's default behaviour open, we may implement this as new datatypes: Declaring variable \mintinline{java}{a} as \mintinline{java}{unchecked short} would implictly narrow to short when needed, so \mintinline{java}{a=b+c;} would not need an explicit cast anymore, while declaring it as a normal \mintinline{java}{short} would.




\section{Simple type definitions}
\label{sec-typedef}
When developing code for a sensor node, the limited resources force us to adopt different design patterns compared to desktop software. In normal Java code we usually rely on objects for type safety and keeping code readable and easy to maintain. But on sensor nodes, objects are expensive and we frequently make use of shorts and ints for a multitude of different tasks for which we would traditionally use objects.

In these situations we often found that our code would be much easier to maintain if we had a way to name new integer types to explicitly indicate their meaning, instead of using many of \mycode{int} or \mycode{short} variables. Having type checking on these types would also add a welcome layer of safety.

\paragraph{Possible solutions}
At a minimum, we should have a way to define simple aliases for primitive types, similar to C's \mycode{typedef}. A more advanced option that fits more naturally with Java, would be to have a strict \mycode{typedef} which also does type checking, so that a value of one user defined integer type cannot be accidentally assigned to a variable of another type, without an explicit cast.




\section{Explicit and efficient inlining}
\label{sec-inlining}
Java method calls are inherently more expensive than C functions. On the desktop, JIT compilers can remove much of this overhead, but a sensor VM does not have the resources for this. We found this often is a problem for small helper functions that are frequently called. As an example, a C version of the xxtea cipher \cite{Wheeler:1998} contains this macro: 
\begin{minted}[fontsize=\footnotesize]{c}
  #define MX (((z>>5^y<<2) + (y>>3^z<<4)) \\
             ^ ((sum^y) + (key[(p&3)^e] ^ z)))
\end{minted}

This macro is called in four places, and is very performance critical. Tools like Proguard \cite{proguard} can be used to inline small methods, but in this case it is larger than Proguard's size threshold. This leaves developers with two unattractive options: either leaving it as a method and accepting the performance penalty, or manually copy-pasting the code, which is error-prone and leads to code that is harder to maintain.

\paragraph{Possible solutions}
The simplest solution would be to have a preprocessor similar to C's. However, such a low level text-based solution may not be the most user friendly solution for developers without a C background.

Another option is to give the developer more control over inlining, which could easily be achieved by adding an \mycode{inline} keyword to force the compiler to inline important methods. These annotations are usually placed at the method level. As an extension, since not all calls may be equally important for performance, it may be useful allow the developer to save code space by placing the \mycode{inline} keyword at a call instead of at the method level to only inline specific, performance sensitive calls.




\section{An optimising compiler}
\label{sec-optimising-javac}
As discussed in previous sections, but listed here again for completeness, Java compilers typically do not optimise the bytecode but translate the source almost as-is. Without a clear performance model it is not always clear which option is faster, and the bytecode is expected to be run by a JIT compiler, which can make better optimisation decisions knowing the target platform and runtime behaviour. However, a sensor node does not have the resources for this and must execute the code as it is received. This leads to significant overhead, for example by repeatedly reevaluating a constant expression in a loop.

\paragraph{Possible solutions}
Even without a clear performance model, some basic optimisations can be done. In the experiments described in Section \ref{sec-evaluation-manual-optimisations}, we described some very conservative optimisations that already result in code twice as fast as the original.




\section{Allocating objects on stack}
\label{sec-no-gc}
In Java anything larger than a primitive value has to be allocated on the heap. This introduces a performance overhead, both for allocating the objects, and the occasional garbage collection run, which may take several thousand cycles.

In our benchmarks we encountered a number of situations where a temporary object was needed. For example, the \mycode{encode} function in the LEC benchmark needs to return two values: \emph{bsi} and the number of bits in \emph{bsi}. In C we do this by passing a pointer. In Java we can wrap both values in an object and either create and return an object from \mycode{encode}, or let the caller create it and pass it as a parameter for \mycode{encode} to fill in.

In code that frequently needs short-lived objects this overhead can be significant, and unpredictable GC runs are a problem for code with specific timing constraints. Besides LEC, we saw similar situations in the CoreMark, MoteTrack and heat tracking benchmarks. The problem is especially serious on a sensor node, where the limited amount of memory means the garbage collector is triggered frequently even if relatively few temporary objects are created.

\begin{listing}[]
	\centering
	\begin{minted}[fontsize=\scriptsize]{java}
    public static short LEC(short[] numbers, Stream stream) {
        BSI bsi = new BSI();      // Allocate bsi only once
        for (...) {
            ...
            compress(ri, ri_1, stream, bsi);
            ...
        }
    }

    private static void compress(short ri, short ri_1, Stream stream, BSI bsi) {
        ...
        encode(di, bsi);          // Pass bsi to encode to return both value and length
        ...
    }

    private static void encode(short di, BSI bsi) {
        ...
        bsi.value = ...           // return value and length by setting object fields
        bsi.length = ...
    }
	\end{minted}
\caption{Avoiding multiple object allocations in the LEC benchmark}
\label{lst-lec-avoiding-object-allocations}
\end{listing}

This overhead can often be reduced by allocating earlier and reusing the same objects in a loop. In Listing \ref{lst-lec-avoiding-object-allocations} we see this implemented for the LEC benchmark, where we use an object to return two values from \mycode{encode} to \mycode{compress}. Instead of creating a new object in each iteration in the \mycode{compress} method where it is needed, we create \mycode{bsi} once, outside of the main loop, and pass the same object to \mycode{compress} multiple times.

This technique of pulling object creation up the call chain can often be used to remove this sort of overhead, and it worked in all four benchmarks mentioned before. However, it gets very cumbersome if the number of objects is more than one or two, or if they need to be passed through multiple layers. Readability is also reduced, since objects that are only needed in a specific location are now visible from a much larger scope.

\paragraph{Possible solutions}
On desktop JVMs, escape analysis \cite{Choi:1999uw, Goetz:2005uy} is used to determine if an object can be safely allocated on the stack instead of the heap, thus saving both the cost of heap allocation, and the occasional garbage collection run triggered by it.

While the analysis of the bytecode required for this is far too complex for a sensor node, it can be done do it offline. Similar to TakaTuka's offline garbage collector analysis, the bytecode can be extended to use the results of this analysis in the VM. In this case, special versions of the \mycode{new} opcodes may be introduced to instruct the VM to place an object in the stack frame instead of the heap, and a field should be added to the method header to tell the VM how much extra space for stack objects needs to be reserved in the stack frame.

There is a risk to doing this automatically. In our sensor node VM, the split between heap and stack memory is fixed, and both are limited. If the compiler automatically puts all objects that could be on the stack in the stack frame instead of the heap, we may end up with an empty heap, and stack overflow. Therefore, it is better to leave this optimisation to the develop by also introducing a new keyword at the language level, so the developer can explicitly indicate which objects go on the stack and which in the heap. Of course escape analysis is still necessary to check at compile time this keyword is only used in places where the object can go on the stack.

\section{Reconsidering advanced language features}
\label{sec-advanced-features}
Finally, we conclude with some discussion on more fundamental language design choices. Many tiny VMs implement some of Java's more advanced features, but we are not convinced these are a good choice on a sensor node.

While features like threads and garbage collection are all useful, they come at a cost. The trade-off when writing sensor node code is significantly different: many of these features are vital to large-scale software development, but the size of sensor nodes programmes is much smaller. And while VM size is not an issue on the desktop, these features are relatively expensive to implement on a sensor node.

In Table \ref{tab-vm-size} we show the code size for some features we discuss below. These were determined by counting the size of functions related to specific features. The actual cost is higher since some, especially garbage collection, also add complexity to other functions throughout the VM. Combined, the features below and the string functions mentioned in Section \ref{sec-std-lib} make up about half the VM.

These features also cause a performance penalty, which is significant when Ahead-of-Time (AOT) compilation is used, and features such as threads and exceptions are much harder in an AOT compiler where we can't implement them in the interpreter loop. This means that if we care about performance and the corresponding reduction in CPU energy consumption, we either have to give them up, or spend considerably more in terms of VM complexity and size.


\subsection{Threads}
As shown in Table \ref{tab-vm-size}, support for threads costs about 10\% of the VM size, if we exclude the string library. In addition, there is the question of allocating a stack for each thread. If we allocate a fixed block, it must be large enough to avoid stack overflows, but too large a block wastes precious RAM. Darjeeling allocates each stack as a linked list of frames on the heap. This is memory efficient, but allocating on the heap is slower and occasionally triggers the GC.

We therefore argue a more cooperative concurrency model is more appropriate where lightweight tasks voluntarily yield the CPU and share a single stack, which is the approach to concurrency chosen by many native code systems, including \emph{t-kernel} and nesC \cite{Gu:2005un, Gay:2003up, Hester:2016je}.

\subsection{Exceptions}
In terms of code size, exceptions are not very expensive to implement in an interpreter, but again, they are harder to implement in an AOT compiler. We also feel the advantage of having exceptions is much lower than the other features mentioned in this section. They could be easily replaced with return values to signal errors.


\subsection{Virtual methods}
It is hard to quantify the overhead of implementing virtual methods since the code for handling them is integrated into several functions. In terms of size it is most likely less than 2KB, but the overhead for resolving a virtual method call is considerable, and an AOT compiler can generate much more efficient code for static calls.

In practice we seldom used virtual methods in sensor node code, but some form of indirect calls is necessary for things like signal handling. It should be possible to develop a more lightweight form of function pointers that can be implemented efficiently. However, the details will require more careful study.


\subsection{Garbage collection}
Finally, garbage collection is clearly the most intrusive one to change. While the first three features could be changed with minor modifications to Java, the managed heap is at its very core.

Still, there are good reasons for considering alternatives. Table \ref{tab-vm-size} shows the GC methods in Darjeeling add up to about 3.5KB, but the actual cost is much higher as many other parts of Darjeeling are influenced by the garbage collector.

Specifically, it is the reason Darjeeling uses a `split-stack' architecture: the operand stack and variables are split into a reference and integer part. This makes it easy for the GC to find live references, but leads to significant code duplication and complexity. When using AOT compilation, the split stack adds overhead to maintain this state, and the extra register we have to reserve as a second stack pointer.




\section{Conclusion}
In this chapter we described a number of issues we encountered over the years while using and developing sensor node VMs. They may not apply to every scenario, but the wide range of the issues we present suggests many applications will be affected by at least some.

Most sensor node VMs already modify the instruction set of the original VM and usually support only a subset of the original language. The issues described here indicate these changes do not go far enough, and we still need to refine our VMs further to make them truly useful in real-world projects.

There are two possible paths to follow: a number of issues can be solved by improving existing Java-based VMs. Staying close to Java has the advantage of being able to reuse existing knowledge and infrastructure.

However some of the issues require more invasive changes to both the source language and VM. If the goal is to run platform independent code safely and efficiently, rather than running Java, we should start from the specific requirements and constraints of sensor node software development. We suspect this would lead us to more lightweight features and more predictable memory models.

For either path, we hope the points presented in this chapter can help in the development better future sensor node VMs.
