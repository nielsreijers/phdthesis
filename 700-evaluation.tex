\chapter{Evaluation}
\label{sec-evaluation}
This dissertation presents a number of techniques to improve the performance of sensor node virtual machines and make them safe, while staying within the constraints set out in Section \ref{sec-CapeVM-goals}. This chapter evaluates to what extent CapeVM meets these goals by measuring its performance and code size overhead for a number of different benchmarks.

First, Section \ref{sec-evaluation-benchmarks} describes our experimental setup, the benchmarks used, and how the source code for these benchmarks was obtained.

Next, Section \ref{sec-evaluation-coremark} uses the largest benchmark to examine the effect of the lack of optimisations done by the standard \mycode{javac} compiler, and the manual optimisations performed on the Java source.

Sections \ref{sec-evaluation-aot-translation-performance}, \ref{sec-evaluation-aot-translation-code-size} and \ref{sec-evaluation-benchmark-details} evaluate the result of the optimisations to the AOT translation process on performance and code size.

Sections \ref{sec-evaluation-const-array} and \ref{sec-evaluation-method-invocation} focus on two specific optimisations: adding support for constant arrays and lightweight method calls.

The cost of adding safety checks is examined in Section \ref{sec-evaluation-safety}, which also compares CapeVM's overhead to existing native code systems that provide safety.

Platform independence is one of the main reasons to use a VM. While CapeVM was only implemented for the ATmega128, Section \ref{sec-evaluation-other-platforms} presents measurements that give an indication of the expected performance on other common sensor node platforms.

Finally, in Section \ref{sec-evaluation-limitations} we discuss the limitations and cost of using a VM, and describe some known hard cases which CapeVM currently does not handle as well.
