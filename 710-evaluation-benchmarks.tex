\section{Benchmarks and experimental setup}
\label{sec-evaluation-benchmarks}
This section describes our experimental setup, and the benchmarks used to evaluate our approach. We describe how the source code for each benchmark was obtained, and describe any relevant details in their implementation.

We use a set of twelve different benchmarks to measure the effect of our optimisations, shown in Table \ref{tbl-benchmarks}

\input{710-tbl-evaluation-benchmarks}

This mix of benchmarks was chosen for several reasons. A number of benchmarks, \mybench{bubble sort}, \mybench{binary search}, \mybench{MD5}, \mybench{FFT}, and \mybench{Outlier detection} were chosen because they are used in various related work, enabling us to compare MyVM to their results.

The first nine or our benchmarks, up to \mybench{RC5} are all small benchmarks consisting of one or two functions that process an array of data. Processing arrays of data is common to many sensor node application, and likely to be performance sensitive. While the actual processing done may not be typical for sensor network (although the \mybench{MoteTrack} benchmark does do a bubble sort), the small size of these benchmarks make them useful to highlights specific behaviours that would be lost in the averages of a larger benchmark.

The \mybench{CoreMark} benchmark is a standard benchmark to measure embedded CPU performance, containing many different methods, enabling us to evaluate the effect of method calls. Since it mixes several kinds of processing, specifically linked list processing, a state machine, and matrix operations, it is a good example of the expected average behaviour, although code focussing on a very specific task may perform differently.

Finally, the \mybench{MoteTrack}, \mybench{heat detection}, and \mybench{LEC} benchmarks are all examples of code that was specifically developed for sensor nodes, and \mybench{FFT} and \mybench{Outlier detection} examples of signal processing operations, which is a common task for sensor nodes.


\subsection{Implementation details}
To make sure our results can be reproduced, we describe the implementation of our benchmarks in this section. For most benchmarks a C version is available in the sources mentioned in Table \ref{tbl-benchmarks}. The sources for \mybench{heat detection}, \mybench{LEC} and \mybench{outlier detection} are not available online, but are listed in the appendices.

The \mybench{bubble sort}, \mybench{heap sort}, \mybench{FFT}, \mybench{binary search}, and \mybench{outlier detection} benchmarks could all be implemented for different data sizes. The effect on the resulting performance is discussed in section \ref{sec-evaluation-other-platforms}. In the rest of this evaluations we used a 16-bit data size. This is large enough for many tasks, for example many analog-to-digital converters output a value that fits in 16 bits, while the memory constraints of sensor nodes mean developers will often be reluctant to use 32-bit variables where 16 bits are sufficient.

Except where listed below, we translate these C sources directly to Java, keeping both implementations as close as possible, and manually optimise the code as described in Section \ref{sec-optimisations-manual-java-source-optimisation}. These optimisations did not affect the performance of the C version significantly, indicating \mycode{avr-gcc} already does similar transformations on the original code.

There are cases where a developer who is aware of the performance characteristics of the VM may choose a different approach than the one used in the C version when developing a Java implementation directly. We discuss some of the issues when translating C to Java for the \mybench{CoreMark} benchmark in Section \ref{sec-evaluation-coremark-unfair-optimisations}, including two choices that would have lead to better performance. However, in most cases we follow the C version as closely as possible to avoid bias by optimising specifically for our VM. We take a bit more liberty for the \mybench{MoteTrack} and \mybench{heat detection} benchmarks, since these could not be directly translated.

Our benchmarks expose some limitations of using a VM instead of native code, which are common to most sensor nodes VMs. Specifically, the lack of support for constant data, high memory overhead for code containing many small objects, and high performance overhead for allocating temporary objects. These are discussed in more detail in Chapter \ref{sec-lessons-from-jvm}, where we also suggest options to solve these limitations. In this section we describe the cases where they forced us to deviate from our preferred direct C to Java mapping.


\subsubsection{MoteTrack}
\label{sec-evaluation-benchmark-implementation-motetrack}
The \mybench{MoteTrack} application uses received signal strength (RSSI) measurements from a number of beacon nodes to do indoor localisation. It contains a small database of reference RSSI signatures, which is stored as a constant array in flash memory for the C version. When implemented as an array in Java, this becomes too large to fit in memory, which forces us to implement the method that reads from this database as a native C method that reads from flash memory instead. This means our benchmark is no longer platform independent.

The constant data stored in this database is a structure of many small struct in an array in C. The memory overhead when translating this directly to Java was too high to run the application, forcing us to make two more modifications. First, the original source has the option to list RSSI signatures at different transmission powers, but the authors note this may not always improve results. The original C code only uses a single transmission power, which results in arrays of a single element that get optimised away at compile time. We replaces these arrays in the Java version with simple variables. Second, we flattened a two element array with RSSI values for different channels into two separated variables to eliminate the overhead for allocating too many small arrays.

These changes do not affect the results of the current version of the code, but we note that while it would be possible to use multiple transmission powers or more channels in the C version, this would require too much memory for the Java version. Thus, while our Java implementation of \mybench{MoteTrack} does execute the same algorithm as the C version, we were force to modify its implementation significantly, which clearly highlights some of the weaknesses of current sensor node VMs.


\subsubsection{Heat detection}
The \mybench{heat detection} application is adapted from code used by a different project in our group for Raspberry Pi devices equiped with an 8x8 heat sensor to track persons or a fire hazard.

It contains two phases: first the heat sensor is calibrated with no heat sources in view to determine the average and standard deviation of the sensor readings. In the next phase the algorithm tracks the position of a person moving within the field of view of the sensor, and detects extreme temperatures that may indicate a fire. We list both phases separately in the evaluation.

We modified the calibration phase to allow it to run on the more resource constrained sensor node, but the resulting calibration data is identical. The code for the detection phase was copied directly from the source used on the Raspberry Pi, only modified to avoid repeatedly allocating temporary objects as described in Section \ref{sec-no-gc}.

Our implementation reads sensor measurements from a table in flash memory using a native call, similar to \mybench{MoteTrack}'s RSSI database. In this case however, we argue this does not mean the application is no longer platform independent since this is only done to allow to be able to run the benchmark in a simulator. A real version would use some library call to read from a sensor.


\subsubsection{LEC compression}
The LEC algorithm is described in detailed pseudo code in \cite{Marcelloni:2009ja}. Our implementation follows this pseudo code as closely as possible, and is listed in Appendix \ref{app-lec-source}. The input is a set of 256 16-bit ECG measurements downloaded from PhysioNet \cite{physionet-ecg-data}.


\subsubsection{FFT}
Both 8-bit and 16-bit versions of the \mycode{fix\_fft.c} exist. In the main evaluation we use the 16-bit version taken from the Harbor source code \cite{sos-operating-system}.

Both versions contain a table of precalculated sine wave values. For the 8-bit version this is a 256 element byte array, which we implemented in Java. For the 16-bit version, the array is a 1024 element array of shorts, which fits in memory, but the JVM code to initialise it becomes too large for flash memory, again highlighting the problem current sensor node VMs have with constant data. The 16-bit version uses a native C call prior to starting the benchmark to initialise the array directly from flash memory.

\subsubsection{Outlier detection}
We implemented the outlier detection algorithm as described in \cite{Kumar:2007ge}:

\begin{displayquote}
"The outlier detector samples a set of sensor values and stores them in a buffer. Once the buffer is filled, it computes the distance between all pairs of samples in the buffer and stores the result in a matrix. Using a distance threshold, the algorithm marks the distance measurements in the matrix that are greater than the threshold. If the majority of the distance measurements for a sensor readings are marked, then the sensor reading is classified as an outlier."
\end{displayquote}

Note that there is in fact no reason to store the distances in a matrix, and having to allocate this matrix limits us to only short arrays of input data. The same result can be calculated directly, without the distance matrix, by examining the samples one at a time, and simply counting the number of other samples with a distance higher than the threshold.

However, since we will use this benchmark to compare our approach to Harbor, we implemented it as described in the paper.

\subsection{Experimental setup}
For each benchmark we implemented both a C and a Java version. We compile these using \mycode{javac} version 1.8.0, ProGuard 5.2.1, and \mycode{avr-gcc} version 4.9.1. The C benchmarks are compiled at optimisation level -O3, the rest of the VM at -Os.

We manually examined the compiled code produced by \mycode{avr-gcc}. While we identified some points where more efficient code could have been generated, except for the lack of some constant shift optimisations discussed in Section \ref{sec-opt-constant-shift}, this did not affect performance by more than a few percent. This leads us to believe \mycode{avr-gcc} is a fair benchmark to compare to.

We run our VM in the cycle-accurate Avrora simulator \cite{Titzer:2005vb}, emulating an ATmega128 processor. We modified Avrora to emit traces of the AOT translation process and of the run-time performance. During AOT translation, our compiler writes to a specific memory address monitored by Avrora to inform it of each step in the process. When running both the C and AOT compiled benchmarks, Avrora tracks the number of cycles spent in each instruction. These traces, combined with debug output from Darjeeling's infuser and disassembled native code allow us to get a detailed view of the performance on a per-instruction basis.

Our main measurement for both code size and performance is the overhead compared to optimised native C. To compare different benchmarks, we normalise this overhead to a percentage of the number of bytes or cpu cycles used by the native implementation: a 100\% overhead means the AOT compiled version takes twice as long to run, or twice as many bytes to store. The exact results can vary depending on factors such as which benchmarks are chosen, the input data, etc., but the general trends are all quite stable.
