\section{Benchmarks and experimental setup}
\label{sec-evaluation-benchmarks}

This section describes our experimental setup, the benchmarks used and how the source code for each benchmark was obtained, and any relevant details in their implementation.

A set of twelve different benchmarks, shown in Table \ref{tbl-benchmarks}, is used to measure the effect of the optimisations and the overhead of safety checks. Table \ref{tbl-evaluation-benchmark-characteristics} shows some key characteristics of these benchmarks: their code size, stack depth, and mix of executed instructions. This mix of benchmarks was chosen for several reasons. Some benchmarks, \mybench{bubble sort}, \mybench{binary search}, \mybench{MD5}, \mybench{FFT}, and \mybench{Outlier detection} were chosen because they are used in various related work, allowing a comparison of CapeVM to these results.

A number of benchmarks are small benchmarks that process arrays of data. While the actual processing done may not be typical for sensor networks (although the \mybench{MoteTrack} application does do a bubble sort), the small size of these benchmarks make them useful to highlights specific behaviours that would be lost in the averages of a larger benchmark.

The \mybench{CoreMark} benchmark is an industry standard benchmark to measure CPU performance. It is a larger benchmark, mixing several kinds of processing: besides array processing in the form of matrix operations, it also contains linked list processing and a state machine. Since \emph{CoreMark} mixes different kinds of processing, it is a good example of the expected average behaviour. The many different methods enables us to evaluate the effect of method calls and show CapeVM can effeciently handle larger, more complex applications.

Finally, \mybench{Outlier detection}, \mybench{LEC}, \mybench{MoteTrack} and \mybench{heat detection} are all code that was specifically developed for sensor nodes, and \mybench{FFT} is a typical signal processing operation, which is a common and potentially expensive task for sensor nodes.

Sensor nodes spend their time and energy on three main tasks: accessing sensor and actuators, communication, and data processing. The first two require interaction with the hardware, so they must be implemented in native code in the VM's standard library. Since native code is not influenced by the performance of the VM, the benchmarks used in this evaluation focus on data processing.

\input{710-tbl-evaluation-benchmarks}
\input{711-tbl-evaluation-benchmark-characteristics}

As noted before, to what extent an application is affected by the VM's slowdown is highly application dependent. The Amulet smart watch system discussed in Chapter \ref{sec-introduction} notes that energy consumed when the CPU is in active mode is significant, and their breakdown of the CPU active time shows most is spent in application code rather than the OS. Similarly, the Mercury motion analysis platform show the energy spent on feature extraction or FFT becomes significant or dominant in the total energy consumption when multiplied by the typical slowdown seen in interpreting VMs.

We argue that some form of array processing will be common in many sensor node applications, and especially so in applications that are significantly affected by the VM's slowdown. First, arrays of data appear in many sensor node application, both in the form of sensor data, and as sent or received radio messages that need to be constructed or parsed. Second, processing such arrays is likely to be a significant part of the total processing time, for the simple reason that looping over an array of elements quickly takes more time than processing a single value.

Finally, we note that compared to complex high performance CPUs, performance on sensor node CPUs is less affected by the exact workload. The simple CPUs found on sensor nodes typically have no caches, and no (ATmega and MSP430) or very short (Cortex M0) pipelines. Thus, factors like branch prediction and cache line alignment that can have a very large impact on more complex CPUs, have no impact on the results presented here. The largest performance difference found in all benchmarks is between a 1.18x slowdown for \mybench{FFT}, and 2.56x slowdown for \mybench{MoteTrack}.

\subsection{Implementation details}
To ensure the results can be reproduced, we describe the implementation of our benchmarks in this section. For most benchmarks a C version is available in the sources mentioned in Table \ref{tbl-benchmarks}. The sources for \mybench{heat detection}, \mybench{LEC} and \mybench{outlier detection} are not available online, but are listed in the appendices.

The \mybench{bubble sort}, \mybench{heap sort}, \mybench{FFT}, \mybench{binary search}, and \mybench{outlier detection} benchmarks could all be implemented for different data sizes. In this evaluation 16-bit data is used. 8-bit data is too small for many tasks, for example the ATmega's has 10-bit ADCs, and the memory constraints of sensor nodes mean developers will often be reluctant to use 32-bit variables where 16 bits are sufficient. Therefore, the middle option is used for the main evaluation, and the effect on the performance of 8-bit or 32-bit data is discussed in Section \ref{sec-evaluation-other-platforms}.

The C versions of these benchmarks were first translated directly to Java, keeping both implementations as close as possible. The result was then manually optimised as described in Section \ref{sec-optimisations-manual-java-source-optimisation}. These optimisations did not affect the performance of the C version significantly, indicating \mycode{avr-gcc} already does similar transformations on the original code.

The C version was followed as closely as possible to avoid bias by optimising specifically for our VM. We take a bit more liberty for the \mybench{MoteTrack} and \mybench{heat calibration} benchmarks, since these could not be directly translated. There are cases where a developer who is aware of the performance characteristics of the VM may choose a different approach than the one used in the C version when developing a Java implementation directly. We discuss some of the issues when translating C to Java for the \mybench{CoreMark} benchmark in Section \ref{sec-evaluation-coremark-non-automatic-optimisations}, including two extra optimisations that could not be done automatically by an optimising compiler, but do lead to better performance.

The benchmarks exposed some limitations of using a VM instead of native code, which are common to most sensor nodes VMs. Specifically, the lack of support for constant data, high memory overhead for code containing many small objects, and high performance overhead for allocating temporary objects. These are discussed in more detail in Chapter \ref{sec-lessons-from-jvm}, where we also suggest options to solve some of these limitations.

\subsubsection{FFT}
Both 8-bit and 16-bit versions of the \mycode{fix_fft.c} implementation exist. In the main evaluation we use the 16-bit version taken from the Harbor source code \cite{sos-operating-system}. Both versions contain a table of precalculated sine wave values, which are stored in flash using the constant array optimisation introduced in Section \ref{sec-opt-constant-arrays}.

\subsubsection{Outlier detection}
We implemented the outlier detection algorithm as described in \cite{Kumar:2007ge}:

\begin{displayquote}
"The outlier detector samples a set of sensor values and stores them in a buffer. Once the buffer is filled, it computes the distance between all pairs of samples in the buffer and stores the result in a matrix. Using a distance threshold, the algorithm marks the distance measurements in the matrix that are greater than the threshold. If the majority of the distance measurements for a sensor readings are marked, then the sensor reading is classified as an outlier."
\end{displayquote}

Note that there is no reason to store the distances in a matrix, and having to allocate this matrix limits us to small arrays of input data. The same result can be calculated directly, without the distance matrix, by examining the samples one at a time, and counting the number of other samples with a distance higher than the threshold.

Because this benchmark will be used to compare CapeVM's safety cost to Harbor's, it was implemented as described in the paper.

\subsubsection{LEC compression}
The LEC algorithm is described in detailed pseudo code in \cite{Marcelloni:2009ja}. Our implementation follows this pseudo code as closely as possible, and is listed in Appendix \ref{app-lec-source}. The input is a set of 256 16-bit ECG measurements downloaded from PhysioNet \cite{physionet-ecg-data}.

\subsubsection{MoteTrack}
\label{sec-evaluation-benchmark-implementation-motetrack}
The \mybench{MoteTrack} application uses received signal strength (RSSI) measurements from a number of beacon nodes to do indoor localisation. It contains a database in flash memory of reference RSSI signatures, stored in a complex structure of many small structs and arrays in C.

The memory overhead when translating this directly to Java was too high to run the application, forcing us to make two modifications. First, MoteTrack has the option to store RSSI signatures at different transmission powers, but the authors note this may not always improve results. The original C code only uses a single transmission power setting, which results in arrays of a single element that get optimised away at compile time. In the Java version these were replaced with simple variables. Second, a two element array with RSSI values for different channels was flattened into two separated variables to eliminate the overhead from allocating too many small arrays.

Again, the constant array optimisation was used to place the data in flash memory. Without this optimisation, it would be impossible to implement this application in Java because the 20 KB database is too large to fit in RAM. Since this only allows arrays of integer types, the single array of nested C structs was split into 7 arrays for the individual fields.

Thus, while our Java implementation of \mybench{MoteTrack} does execute the same algorithm as the C version, we were force to modify its implementation significantly, which clearly highlights some of the weaknesses of current sensor node VMs. These changes do not affect the results of the current version of the code, but we note that while it would be possible to use multiple transmission powers or more channels in the C version, this would require too much memory for the Java version.

\subsubsection{Heat detection}
The \mybench{heat detection} application is adapted from code used by a different project in our group to track persons and fire hazards using Raspberry Pi devices equipped with an 8x8 heat sensor.

It contains two phases: first the heat sensor is calibrated with no heat sources in view to determine the average and standard deviation of the sensor readings. In the next phase the algorithm tracks the position of a person moving within the field of view of the sensor, and detects extreme temperatures that may indicate a fire. In the evaluation both phases are listed separately.

The calibration phase was modified to allow it to run on the more resource-constrained sensor node, but the results of the calibration are identical. The code for the detection phase was copied directly from the source used on the Raspberry Pi, but modified slightly to avoid repeatedly allocating temporary objects as described in Section \ref{sec-no-gc}.

Our implementation reads sensor measurements using a native call to read from a table in flash memory, simulating to how a real version would use a library call to read from a sensor.


\subsection{Experimental setup}
Each benchmark is implemented as a C and a Java version. We compile these using \mycode{javac} version 1.8.0, ProGuard 5.2.1, and \mycode{avr-gcc} version 4.9.1. The C benchmarks are compiled at optimisation level -O3, the rest of the VM at -Os.

A manual examination of the compiled code produced by \mycode{avr-gcc} revealed some points where more efficient code could have been generated. But with the exception of the lack of some constant shift optimisations discussed in Section \ref{sec-opt-constant-shift}, these did not affect performance by more than a few percent. This leads us to believe \mycode{avr-gcc} is a fair benchmark to compare to.

Each benchmark was run in the cycle-accurate Avrora simulator \cite{Titzer:2005vb}, emulating the ATmega128 processor. Avrora was modified to emit traces of the AOT translation process and of the run-time performance. During AOT translation, the VM writes to a specific memory address monitored by Avrora to inform it of each step in the process. When running both the C and AOT compiled benchmarks, Avrora tracks the number of cycles spent in each instruction. These traces, combined with debug output from the infuser and disassembled native code provide a detailed view of the performance on a per-instruction basis.

The main measure for both code size and performance is the overhead compared to optimised native C. To compare different benchmarks, this overhead is normalised to a percentage of the number of bytes or cpu cycles used by the native implementation: a 100\% overhead means the AOT compiled version takes twice as long to run, or twice as many bytes to store. The exact results can vary depending on factors such as which benchmarks are chosen, the input data, etc., but the general trends are all quite stable.
